---
template: BlogPost
path: /cs224n
date: 2020-02-20T14:59:36.571Z
title: Improving QA Model Consistency with Natural Language Inference
subtitle: CS 224N Project
thumbnail: /assets/image-5.jpg
---
# Summary
Large language models, such as BERT, are usually pretty good in terms of accuracy. However, one of the main themes in their inaccuracies is *inconsistencies*. Essentially, we often find that language models believe statements A and B, but if A and B together imply C, sometimes the model won't believe C. For instance, we could query a model: "Is a buffalo an animal?" and "Do all animals drink water?" and it might correctly answer "Yes" to both. However, if we then ask it "Do buffaloes drink water?" it might incorrectly answer "No", even though the first two statements imply the last. 

This is a problem because it suggests our models don't actually have consistent belief storage; indeed, models simply memorizing or encoding syntax instead of actual meaning is a big issue. However, it's infeasible to manually go through a model's beliefs and fix incorrect beliefs one by one. Instead, in this paper my partners (Kevin Yang and Patrick Liu) take on the task of using a pre-trained natural language inference (NLI) model to detect beliefs that are inconsistent with each other, and then use that to determine which beliefs are incorrect and which beliefs are correct.

You can check out the full paper [here](/Users/pengwah/Ian/College/Career/personal-site/assets/cs224n_pdf.pdf)